---
title: "Modern Data Mining - Final Project"
subtitle: "Predicting Dementia Among Elderly People"
author:
- Jia Xu
- Yuqin Zhang
- Zejia Cai
output:
  html_document:
    code_folding: show
    highlight: haddock
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 4
    toc_float: yes
  pdf_document:
    number_sections: no
    toc: no
    toc_depth: '4'
  word_document:
    toc: yes
    toc_depth: '4'
urlcolor: blue
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.height=4, fig.width=6, warning = F)
if (!require("pacman")) install.packages("pacman")
pacman::p_load(data.table,randomForest, tree, ISLR, rpart, gbm, rpart.plot, caret, car, MASS, rattle, pROC, partykit, ggplot2, glmnet, leaps, dplyr, keras, neuralnet, imager, ranger, elasticnet,factoextra, stargazer,keras)
```



# I. Abstract

Alzheimer’s disease (AD) is the most common type of dementia among elderly people that leads to memory loss. More severely, it can affect the patient to carry out daily activities. AD is a progressive disease and usually starts slowly, but changes in the brain can begin many years before the appearance of first symptoms. Age has also shown to be associated with the risk of developing AD.

Our [data](https://www.kaggle.com/datasets/jboysen/mri-and-alzheimers?select=oasis_longitudinal.csv) consists of Magnetic Resonance Imaging information for demented and nondemented elderly adults. In this study, we present several ways of building classifiers to predict whether a subject will be diagnosed to develop dementia.

The data was released by Open Access Series of Imaging Studies (OASIS).

# II. Description of the Data

The dataset is a longitudinal collection of MRI scan history of 150 elder adults aged between 60 and 98. Subjects may be scanned more than once, and there are 373 imaging sessions recorded in total. The following table showcases the features that the original data contains:

|  Variable Name   | Description |
|-------|-------|
| Subject.ID | The unique identification of each subject |
| MRI.ID | The unique identification of each scan session |
| M.F | Gender of the subject |
| Age | Age of the subject |
| Hand | Dominant Hand |
| EDUC | Years of education |
| SES | Socialeconomic status |
| MMSE | Mini Mental State Examination Score |
| eTIV | Estimated total intracranial Volume |
| nWBV | Normalized whole brain volume |
| ASF | Atlas scaling factor |
| MR.Delay | MR Delay Time |
| CDR | Clinical Dementia Rating |

Here are some more detailed explanations of the terms mentioned above:

Mini Mental State Examination (MMSE): This is a 30-point questionnaire which has been widely adopted to measure cognitive functions among elderly people.

Estimated total intracranial volume (eTIV): This is an estimated value of the maximum pre-morbid brain volume.

Atlas scaling factor (ASF): This is a volume-scaling factor that standardizes the head size based on differences in human anatomy.

MR Delay: A delayed MR is performed a few minutes after the injection of the contrast agent. The delayed contrast enhancement might reveal different biological information.

Clinical Dementia Rating: This is a globally accepted measure of the overall severity of dementia. The score has the following 5 values:

0 - Normal

0.5 - Very Mild Dementia

1 - Mild Dementia

2 - Moderate Demantia

3 - Severe Demantia


## Target Value

We will predict whether a patient has developed dementia based on the Clinical Dementia Rating. If a patient has CDR score = 0, the subject has no AD, thus receiving a label 0. If the score > 0, the subject has developed AD, thus will be marked with 1.

# III. Data Cleaning and Preparation

We combine the cross sectional dataset with the longitudinal dataset.

```{r}
data_cross <- read.csv("data/oasis_cross-sectional.csv")

# all subjects are right-handed. Delay is all empty
data_cross <- data_cross %>% select(-c(Hand, Delay))

# to match the name of data_long
colnames(data_cross)[which(names(data_cross) == "Educ")] <- "EDUC"
head(data_cross)
```


```{r, include = F}
data_long <- read.csv("data/oasis_longitudinal.csv")

# hand only has one unique value. 
# Group has 3 categories: nondemented, demented, converted. Not interested in converted. Will create our own label.
# Visit, MR.Delay information are not contained in data_cross
data_long <- data_long %>% select(-c(Hand,Group,Subject.ID,Visit, MR.Delay))
colnames(data_long)[which(names(data_long) == "MRI.ID")] <- "ID"
head(data_long)
```


```{r}
# combine data_cross and data_long
data <- rbind(data_cross, data_long)

## check the number of columns which have missing values
names(data)[apply(data, 2, function(x) any(is.na(x)))] # "EDUC" "SES"  "MMSE" "CDR" 
sum(is.na(data$EDUC)) # 201
sum(is.na(data$MMSE)) # 203
sum(is.na(data$SES)) # 239
sum(is.na(data$CDR)) # 201
```

```{r}

# CDR is the target value. Drop columns where CDR is empty. Fill NA for the other three variables.
data <- data[!is.na(data$CDR),]
# EDUC: 18 different possible values, fill NA with mode
table(data$EDUC)
data[is.na(data$EDUC),"EDUC"] <- 12

# column "SES" refers to social-economic status. It's a categorical variable. fill NA with the mode
table(data$SES)
data[is.na(data$SES),"SES"] <- 2

# column "MMSE" refers to mini-mental state exam score, fill NA with mean
unique(data$MMSE)
mean(na.omit(data$MMSE))
data[is.na(data$MMSE),"MMSE"] <- 27
```



```{r, include = F}
# change gender to factor. recode
colnames(data)[which(names(data) == "M.F")] <- "Gender"
data$Gender <- as.factor(data$Gender)
# data$Gender <- as.factor(ifelse(data$Gender=="F", 0, 1))

# create label
data$AD <- as.factor(ifelse(data$CDR==0, 0, 1))
data <- data %>% select(-CDR)
```

```{r, include = F}
summary(data)
```



# IV.Exploratory Data Analysis

```{r echo = F}
data %>% 
  ggplot(aes(x=Age, fill = Gender)) +
  geom_bar(position='dodge') +
  labs(title = "Demographic Information of Subjects",
       x = "Age",
       y = "Number of Patients") +
  theme_light()
```

## TODO:Change size legend! scale_size_manual doesn't work??


AD is associated with lower mini mental state examination score. 
Age does not pose a significant influence on the examination score and the diagnosis of AD.
Two subjects are having significantly lower mini mental state examination score. They are also visiting the hospital very often.
```{r echo = F}
data %>% 
  ggplot(aes(x=MMSE,y=Age, color = AD, size = 2)) +
  geom_jitter(alpha = 0.8) +
  labs(title = "Mini Mental State Examination Score, Age and AD diagnosis",
       x = "Mini Mental State Examination Score") +
  theme_light()
```

##TODO: more visualizations?



# Feature engineering maybe?


# V. Model Building

## Data Splitting

We split the data into three sets: training, testing and validation. The training set will be used to fit a model; the testing set will be used report a model's effectiveness; and the validation set will be held until the end to evaluate our final model.

```{r, include = F}
# train-validation-test split
N <- length(data$AD)
n1 <- floor(0.7 * N)
n2 <- floor(0.15 * N)

set.seed(10)
idx_train <- sample(N,n1)
idx_no_train <- which(!seq(1:N) %in% idx_train)
idx_test <- sample(idx_no_train, n2)
idx_val <- which(!idx_no_train %in% idx_test)
 
# identification information, i.e. encounter_id and patient_nbr, should not be included in the model
data.train <- data[idx_train,] %>% select(-c(ID))
data.test <- data[idx_test,]  %>% select(-c(ID))
data.val <- data[idx_val,] %>% select(-c(ID))


# scale training set
# data.train[, !names(data.train) %in% c("Gender", "AD")] <- scale(data.train[, !names(data.train) %in% c("Gender", "AD")], center = TRUE, scale = TRUE)
```

## Model 1: LASSO Logistic Regression Model

- Model Fit

We first fit a logistic regression model. We selected a sparse model by using LASSO regularization technique. We used **cv.glmnet()** function to implement cross validation.

The criteria is set to be deviance and 10-fold Cross Validation is applied. We chose the set of variables which give the smallest cross-validated error. The variables selected are: Gender, Age, EDUC, SES, MMSE, eTIV and nWBV.

```{r, echo = F}
X <- model.matrix(AD~., data.train)[,-1]
Y <- data.train[,length(data.train)]

set.seed(10)
fit1.cv <- cv.glmnet(X, Y, alpha=1, family="binomial", nfolds = 10, type.measure = "deviance")
plot(fit1.cv)
```


```{r}
coef.1se <- coef(fit1.cv, s = "lambda.min")
coef.1se <- coef.1se[which(coef.1se != 0),]
vars <- rownames(as.matrix(coef.1se))
vars
```


```{r}
fit.logit <- glm(AD~Gender+ Age + EDUC + SES + MMSE + eTIV + nWBV, family=binomial, data=data.train)
summary(fit.logit)
```



- Fine Tuning

Based on this model, we refit the model using backward selection to drop any variables with significance level p < 0.05. In the end, only three variables remained, which are Gender, MMSE and nWBV. In fact, this is the same set of variable that corresponds to the largest value of $\lambda$ such that the cross-validated error is within 1 standard error of the minimum cross-validated error.

```{r}
fit.logit.final <- glm(AD~Gender+ MMSE + nWBV, family=binomial, data=data.train)
summary(fit.logit.final)
```

- Analysis

Based on the summary table of the model, the logit function is given by:
$$
\begin{align}
logit(P(AD=1 \mid Gender, MMSE, nWBV)) 
&= log(\frac{P(AD=1 \mid Gender, MMSE, nWBV)}{P(AD=0 \mid Gender, MMSE, nWBV)}) \nonumber\\
&= 36.38 + 0.82 \cdot Gender(Male) - 0.95 \cdot MMSE - 14.11 \cdot nWBV
\end{align}
$$

where
$$
\begin{split}
P(AD=1 \mid Gender, MMSE, nWBV) = \frac{exp(36.38 + 0.82 \cdot Gender(Male) - 0.95 \cdot MMSE - 14.11 \cdot nWBV)}{1 + exp(36.38 + 0.82 \cdot Gender(Male) - 0.95 \cdot MMSE - 14.11 \cdot nWBV)}
\end{split}
$$

Here, we will assume that it costs equally to mislabel a subject to be AD as it does to mislabel a non-AD. Thus, we will set the threshold to be 0.5. That is, 

$$
\hat{AD} = 1 \text{ if  } \hat P(AD=1 \mid Gender, MMSE, nWBV) > 0.5
$$


```{r, include = F}
#use test dataset to estimate misclassification error
fit.logit.final.test <- predict(fit.logit.final,data.test,type = "response")
fitfinal.pred <- as.factor(ifelse(fit.logit.final.test > 1/2, "1", "0"))
```




Using the test data to make classifications based on the above-mentioned criteria, we obtained the following confusion matrix.

```{r}
cm.test <- table(fitfinal.pred, data.test$AD)
cm.test
```


|     | $Y=0$ | $Y=1$ |
|-------|-------|-------|
| $\hat{Y}=0$ | 41 | 11 |
| $\hat{Y}=1$ | 7 | 32 |


The misclassification error rate is 0.198.
```{r}
log.error.test <- mean(fitfinal.pred != data.test$AD)
log.error.test
```

```{r}
sensitivity <- cm.test[2,2]/sum(cm.test[,2])
sensitivity

specificity <- cm.test[1,1]/sum(cm.test[,1])
specificity

fdr <- cm.test[2,1] / sum(cm.test[2,]) # false discovery rate
fdr

positive.pred <- cm.test[2,2] / sum(cm.test[2,]) # positive prediction
positive.pred
```


```{r, echo = F}

# fit.logit.final.roc <- roc(data.train$AD, fit.logit.final$fitted)
# fit.logit.final.roc <- roc(data.test$AD, fitfinal.pred)

plot(1-fit.logit.final.roc$specificities, fit.logit.final.roc$sensitivities, 
     col = "red", lwd = 3, type = "l",
    xlab = "False Positive",
    ylab = "True Positive (Sensitivity)",
    main = "Logistic Regression Model: ROC Curve")

legend("bottomright", c(paste0("AUC = ", round(fit.logit.final.roc$auc,2))),
       col = c("red"))
```



## Model 2 Random Forest 
## TODO: 要不要再加个用gini的

Second model we built is a random forest model. We first set mtry to be equal to square root of p (10) and num.trees to be 500. We use default setting split criterion: misclassification error.

- Tuning Parameters
Using the randomForest()

```{r, echo = F}
set.seed(1)
fit.rf <- randomForest(AD~., data.train, mtry = 3, 
                    num.trees = 500)

# Three curves of MCE of 1's, 0's and  overall.
plot(fit.rf)
legend("topright", colnames(fit.rf$err.rate), col=1:3, cex=0.8, fill=1:3)
```

By plotting the error rate vs number of trees, we find that 250 trees may be needed to settle the OOB testing errors.
 
By setting `ntree=250`, we want to compare effects of different mtry.
So we loop mtry from 1 to 10 and return the testing OOB errors for each of them
In the end, we decide to use `mtry=2` which gives the minimum error rate.

```{r, echo = F}
set.seed(1)
rf.error <- 1:10  
for (p in 1:10)  
{
  fit.rf <- randomForest(AD~., data.train, mtry = p, 
                    num.trees = 250)
  rf.error[p] <- fit.rf$err.rate[250]  # collecting oob error rate based on 250 trees
}
# rf.error   

plot(1:10, rf.error, pch=16,
     main = "Testing errors of mtry with 250 trees",
     xlab="mtry",
     ylab="OOB mse of mtry")
lines(1:10, rf.error)
```

- Analysis

```{r, echo = F}
set.seed(1)
fit.rf.final <- randomForest(AD~., data.train, mtry = 2, 
                    num.trees = 250,importance = T)
varImpPlot(fit.rf.final)

```

From the above plot, we get to know that the MMSE is the most important variable in this model.

```{r, include = F}
fit.rf.final.prob <-predict(fit.rf.final, data.test, type = 'prob')
fit.rf.final.pred<-as.factor(ifelse(fit.rf.final.prob[,2]>1/2, "1", "0"))
fit.rf.test.err <-mean(data.test$AD != fit.rf.final.pred)
fit.rf.test.err
```

```{r}
fit.rf.final.roc <- roc(data.test$AD, fit.rf.final.prob[,2])

plot(1-fit.rf.final.roc$specificities, fit.rf.final.roc$sensitivities, 
     col = "red", lwd = 3, type = "l",
    xlab = "False Positive",
    ylab = "True Positive (Sensitivity)",
    main = "Final Model: ROC Curve")

legend("bottomright", c(paste0("AUC = ", round(fit.rf.final.roc$auc,2))),
       col = c("red"))
```


Using 1/2 as threshold to determine the class of the instances in the test dataset, the misclassification rate is 0.131.
and AUC is 0.95. The confusion matrix is showing below and we can see that the model's ability to predict positive class or negative class is balanced.

```{r, echo = F}
cm.val <- table(fit.rf.final.pred, data.test$AD)
cm.val
```

|     | $Y=0$ | $Y=1$ |
|-------|-------|-------|
| $\hat{Y}=0$ | 43 | 7 |
| $\hat{Y}=1$ | 5 | 36 |

## Model 3: Gradient Boosting

We then applied a gradient boosted model with gaussian loss function and 10000 iterations were performed.
```{r}
# for reproducibility
set.seed(123)

# train GBM model
fit.gbm <- gbm(AD~., distribution = "gaussian", 
                data = data.train,
                n.trees = 10000, 
                interaction.depth = 1, 
                shrinkage = 0.001,
                cv.folds = 10, 
                verbose = FALSE, 
                n.cores = NULL)
print(fit.gbm)
summary(fit.gbm)
```

The best cross-validation iteration was 5970
There were 10 predictors of which 10 had non-zero influence.

Here, we see that the minimum CV RMSE is 0.3552643 (this means on average our model is about 0.3552643 off from the actual AD diagnosis) but the plot also illustrates that the CV error is still decreasing at 4012 trees.
```{r}
# get MSE and compute RMSE
sqrt(min(fit.gbm$cv.error))
gbm.perf(fit.gbm, method = "cv")
min_MSE <- which.min(fit.gbm$cv.error)
min_MSE
```

- Tuning parameter

```{r,include=FALSE}
# By increase the learning rate to take larger steps down the gradient descent, reduce the number of trees (since we are reducing the learning rate), and increase the depth of each tree from using a single split to 2 splits. This model achieves a slightly lower RMSE than our initial model with only 327 trees.
# for reproducibility
set.seed(123)

# train GBM model
fit.gbm1 <- gbm(AD~., distribution = "gaussian", 
                data = data.train,
                n.trees = 5000,
                interaction.depth = 2,
                shrinkage = 0.01,
                cv.folds = 10,
                n.cores = NULL, 
                verbose = FALSE)  
print(fit.gbm1)
summary(fit.gbm1)
# find index for n trees with minimum CV error
min_MSE1 <- which.min(fit.gbm1$cv.error)
min_MSE1
# get MSE and compute RMSE
sqrt(fit.gbm1$cv.error[min_MSE1])

# plot loss function as a result of n trees added to the ensemble
gbm.perf(fit.gbm1, method = "cv")
```
We then used the grid search method, which iterates over every possible combination of hyperparameter values and allows us to determine which combination performs best.

To conduct a manual grid search, we first create a grid of hyperparameter combinations. We'll look at 81 models with different learning rates and tree depths.
```{r}
# create hyperparameter grid
hyper_grid <- expand.grid(shrinkage = c(.001, .01, .1),
                          interaction.depth = c(1, 3, 5),
                          n.minobsinnode = c(5, 10, 15),
                          bag.fraction = c(.65, .8, 1), 
                          optimal_trees = 0,
                          min_RMSE = 0)

# total number of combinations
nrow(hyper_grid)
```
Instead of performing 10-fold cross validation, we can speed up our tuning process by training on 75% of the training dataset and evaluate performance on the remaining 25%.
```{r}
random_index <- sample(1:nrow(data.train), nrow(data.train))
random_train <- data.train[random_index, ]

# grid search 
for(i in 1:nrow(hyper_grid)) {
    # reproducibility
    set.seed(123)
    
    # train model
    fit.gbm.tune <- gbm(AD~.,distribution = "gaussian",
                    data = random_train,n.trees = 5000,
                    interaction.depth = hyper_grid$interaction.depth[i],
                    shrinkage = hyper_grid$shrinkage[i],
                    n.minobsinnode = hyper_grid$n.minobsinnode[i],
                    bag.fraction = hyper_grid$bag.fraction[i],
                    train.fraction = .75, # speed up
                    n.cores = NULL,
                    verbose = FALSE)
    # add min training error and trees to grid
    hyper_grid$optimal_trees[i] <- which.min(fit.gbm.tune$valid.error)
    hyper_grid$min_RMSE[i] <- sqrt(min(fit.gbm.tune$valid.error))
  }

hyper_grid %>% 
    dplyr::arrange(min_RMSE) %>%
    head(10)
```
Our best model is with an RMSE of 0.1806114, which is much better than our first model's.

```{r}
# for reproducibility
set.seed(123)

# train final model
fit.gbm.final <- gbm(AD~ .,
                    distribution = "gaussian",
                    data = data.train,
                    n.trees = 62,
                    interaction.depth = 3,
                    shrinkage = 0.05,
                    n.minobsinnode = 10,
                    bag.fraction = .80, 
                    train.fraction = .75,
                    n.cores = NULL, 
                    verbose = FALSE)  
# print(fit.gbm.final)
summary(fit.gbm.final,
        cBars = 10,
        method = relative.influence, 
        las = 2)
```
- Predict

```{r}
fit.gbm.pred <- predict(object = fit.gbm.final, 
                        data.test,
                        type = "response",
                        fit.gbm.final$n.trees)

fit.gbm.pred <- ifelse(fit.gbm.pred >= 1, "1", "0")

# caret::RMSE(fit.gbm.pred, data.test$AD)
table(fit.gbm.pred,data.test$AD)
```

## Model 4 Deep Learning 这个应该有点问题

```{r}
p = dim(X)[2]
deep_model <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = c(p)) %>% 
  layer_dropout(0.5) %>% 
  layer_dense(units = 2, activation = "softmax") # output

deep_model %>% compile(
  optimizer = "rmsprop",
  loss = "sparse_categorical_crossentropy",
  metrics = c("accuracy")
)
```

```{r}
fit.deep <- deep_model %>% fit(
  X,
  as.matrix(as.numeric(as.character(Y))),
  epochs = 100,
  validation_split = .15 # set 15% of the data3_xtain, data3_ytrain as the validation data
)

plot(fit.deep)
```



```{r}
deep_model %>% fit(
  X,
  as.matrix(as.numeric(as.character(Y))),
  epochs = 50,
)
deep_model%>% evaluate(model.matrix(AD~.,data.test)[,-1],as.matrix(as.integer(as.character(data.test$AD))))
```

```{r}
pred.prob <- deep_model %>% predict(data.test)
pred.prob
```

```{r}
data.test[,]
```



## Enesemble Model
 
## Final Model



# Conclusion

# Reference 

Fulton, L.V.; Dolezel, D.; Harrop, J.; Yan, Y.; Fulton, C.P. Classification of Alzheimer’s Disease with and without Imagery Using Gradient Boosted Machines and ResNet-50. Brain Sci. 2019, 9, 212. https://doi.org/10.3390/brainsci9090212

Knight Alzheimer Disease Research Center, CDR Scoring Table: https://knightadrc.wustl.edu/professionals-clinicians/cdr-dementia-staging-instrument/cdr-scoring-table/
